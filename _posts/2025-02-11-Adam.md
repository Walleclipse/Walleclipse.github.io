---
layout: post
title:  "Revisiting the Initial Steps in Adaptive Gradient Descent Optimization"
date:   2025-02-11 18:08:39 +00:00
image: /images/AdamInit.png
categories: research
author: "Abulikemu Abuduweili"
authors: "<strong>Abulikemu Abuduweili</strong>, Changliu Liu"
venue: " Conference on Parsimony and Learning (CPAL)"
location: "Stanford, CA, USA"
arxiv: https://arxiv.org/abs/2412.02153 
code: https://github.com/Walleclipse/Adam_Initialization/ 
---

Adam is everywhere in deep learning, but its default initialization has a subtle flaw that hurts generalization. We traced the problem to the second-order moment estimate starting at zeroâ€”a seemingly innocent choice with real consequences. Simple fixes like data-driven or random initialization yield meaningful performance gains. Sometimes the biggest improvements come from revisiting the basics.