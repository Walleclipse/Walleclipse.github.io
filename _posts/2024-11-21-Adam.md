---
layout: post
title:  "Revisiting the Initial Steps in Adaptive Gradient Descent Optimization"
date:   2024-11-21 18:08:39 +00:00
image: /images/AdamInit.png
categories: research
author: "Abulikemu Abuduweili"
authors: "<strong>Abulikemu Abuduweili</strong>, Changliu Liu"
venue: " Annual Workshop on Optimization for Machine Learning (OPT)"
location: "Vancouver, Canada"
arxiv: https://arxiv.org/abs/2412.02153 
---


While Adam is known for its faster convergence, it often exhibits poorer generalization compared to SGD. In this work, 
we show the standard initialization of the second-order moment estimation ($v_0 =0$) of Adam as a significant factor contributing 
to these limitations. We introduce simple yet effective solutions: initializing the second-order moment estimation with non-zero values, 
using either data-driven or random initialization strategies. 