---
layout: post
title:  "Revisiting the Initial Steps in Adaptive Gradient Descent Optimization"
date:   2024-11-21 18:08:39 +00:00
image: /images/AdamInit.png
categories: research
author: "Abulikemu Abuduweili"
authors: "<strong>Abulikemu Abuduweili</strong>, Changliu Liu"
venue: " Annual Workshop on Optimization for Machine Learning (OPT)"
location: "Vancouver, Canada"
arxiv: https://openreview.net/forum?id=Xe0dVlWU2q&referrer=%5Bthe%20profile%20of%20Abulikemu%20Abuduweili%5D(%2Fprofile%3Fid%3D~Abulikemu_Abuduweili2) 
---


While Adam is known for its faster convergence, it often exhibits poorer generalization compared to SGD. In this work, we demonstrate that this limitation is closely tied to the standard initialization of the second-order moment estimate, v_0=0. 
To address this, we propose a simple yet effective modification: initializing the second-order moment estimate with non-zero random values. 
This non-zero initialization significantly enhances Adam's performance, enabling it to achieve results comparable to those of many recently 
proposed variants of adaptive gradient optimization methods.