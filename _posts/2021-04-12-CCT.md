---
layout: post
title:  "Escaping the Big Data Paradigm with Compact Transformers"
date:   2021-04-12 18:08:39 +00:00
image: /images/CCT.png
categories: research
author: "Abulikemu Abuduweili"
authors: "Ali Hassani, Steven Walton, Nikhil Shah, <strong>Abulikemu Abuduweili</strong>,  Jiachen Li, Humphrey Shi"
venue: "CVPR Workshop on Learning from Limited and Imperfect Data  <strong>[Invited Talk]</strong>"
arxiv: https://arxiv.org/abs/2104.05704 
code: https://github.com/SHI-Labs/Compact-Transformers 
video: https://www.youtube.com/watch?v=AEWhf_hMBgs 
blog: https://medium.com/pytorch/training-compact-transformers-from-scratch-in-30-minutes-with-pytorch-ff5c21668ed5
highlight: true
---


This paper challenges the notion that transformers require large datasets, demonstrating that with appropriate size and tokenization, they can match or surpass CNNs on small datasets. The Compact Convolutional Transformer (CCT) achieves 98% accuracy on CIFAR-10 with only 3.7M parameters, outperforming other transformers in efficiency and rivaling ResNet50 at 15% of its size.