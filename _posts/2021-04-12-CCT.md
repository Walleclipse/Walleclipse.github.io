---
layout: post
title:  "Escaping the Big Data Paradigm with Compact Transformers"
date:   2021-04-12 18:08:39 +00:00
image: /images/CCT.png
categories: research
author: "Abulikemu Abuduweili"
authors: "Ali Hassani, Steven Walton, Nikhil Shah, <strong>Abulikemu Abuduweili</strong>,  Jiachen Li, Humphrey Shi"
venue: "CVPR Workshop on Learning from Limited and Imperfect Data  <strong>[Invited Talk]</strong>"
arxiv: https://arxiv.org/abs/2104.05704 
code: https://github.com/SHI-Labs/Compact-Transformers 
video: https://www.youtube.com/watch?v=AEWhf_hMBgs 
blog: https://medium.com/pytorch/training-compact-transformers-from-scratch-in-30-minutes-with-pytorch-ff5c21668ed5
highlight: true
---


In this paper, we dispel the myth that transformers are “data hungry” and therefore can only be applied to large sets of data. 
We show for the first time that with the right size and tokenization, transformers can perform head-to-head with state-of-the-art CNNs on small datasets, 
often with better accuracy and fewer parameters.
Our Compact Convolutional Transformer (CCT) achieves 98% accuracy on CIFAR-10 with only 3.7 million parameters, 
making it over 10 times smaller than other transformers and just 15% the size of ResNet50, while maintaining comparable performance.